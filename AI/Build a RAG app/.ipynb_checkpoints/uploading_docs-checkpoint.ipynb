{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary style=\"display:list-item; font-size:16px; color:blue;\">Jupyter Help</summary>\n",
    "    \n",
    "Having trouble testing your work? Double-check that you have followed the steps below to write, run, save, and test your code!\n",
    "    \n",
    "[Click here for a walkthrough GIF of the steps below](https://static-assets.codecademy.com/Courses/ds-python/jupyter-help.gif)\n",
    "\n",
    "Run all initial cells to import libraries and datasets. Then follow these steps for each question:\n",
    "    \n",
    "1. Add your solution to the cell with `## YOUR SOLUTION HERE ## `.\n",
    "2. Run the cell by selecting the `Run` button or the `Shift`+`Enter` keys.\n",
    "3. Save your work by selecting the `Save` button, the `command`+`s` keys (Mac), or `control`+`s` keys (Windows).\n",
    "4. Select the `Test Work` button at the bottom left to test your work.\n",
    "\n",
    "![Screenshot of the buttons at the top of a Jupyter Notebook. The Run and Save buttons are highlighted](https://static-assets.codecademy.com/Paths/ds-python/jupyter-buttons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z24J9NAx-aJY"
   },
   "source": [
    "**Setup**\n",
    "\n",
    "Run the setup cell to import Chroma and Langchain's `RecursiveCharacterTextSplitter`.  The variable `files` gives us some details about the two files we will upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ui7ngtfA-g0u",
    "tags": [
     "setup"
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chromadb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d307a3b437ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_splitter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRecursiveCharacterTextSplitter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m files = [\n\u001b[0;32m      5\u001b[0m     {\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'chromadb'"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "files = [\n",
    "    {\n",
    "        \"title\": \"Anthropology of Food\",\n",
    "        \"source_url\": \"https://openstax.org/details/books/introduction-anthropology\",\n",
    "        \"filename\": \"anthro_food.txt\"\n",
    "  },\n",
    "  {\n",
    "        \"title\": \"Anthropology of Art, Music, and Sport\",\n",
    "        \"source_url\": \"https://openstax.org/details/books/introduction-anthropology\",\n",
    "        \"filename\": \"anthro_art.txt\"\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-KmL1zi8iUH"
   },
   "source": [
    "#### Checkpoint 1/4\n",
    "\n",
    "We've initialized a Chroma client using `chromadb.PersistentClient(\"./\")` to persist the Chroma vector database. Now, create a collection with the variable name `collection` by using the Chroma client's `get_or_create_collection()` method with the following specifications: \n",
    "- Set the `name` argument to a new collection named `\"RAG_Assistant\"`. \n",
    "- Set metadata parameter to specify cosine similarity as the distance metric for the HNSW index.\n",
    "\n",
    "**Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "K1ZjyAS2G76Z",
    "outputId": "77ea3e46-041d-4232-befc-80ac176b593e",
    "tags": [
     "cp1"
    ]
   },
   "outputs": [],
   "source": [
    "# Instantiate a Chroma persistent client\n",
    "client = chromadb.PersistentClient(\"./\")\n",
    "\n",
    "\n",
    "## YOUR SOLUTION HERE ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eQjqQl7APob"
   },
   "source": [
    "#### Checkpoint 2/4\n",
    "Next, we will create a text splitter instance using the `RecursiveCharacterTextSplitter()` class in LangChain. Define the instance as `text_splitter` with the following arguments: \n",
    "- `separators` set to `[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \"]`\n",
    "- `chunk_size` set to `1500`, \n",
    "- `chunk_overlap`set to`200`.  \n",
    "\n",
    "Once your text splitter is correctly defined, it will use the file in files, `anthro_food.txt`, as the content and create a list of document chunks.  The first document will be print to verify your code worked correctly.\n",
    "\n",
    "**Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6DZ3YoV9OM-X",
    "outputId": "b45929a2-3579-4894-97ea-1e1bca81c536",
    "tags": [
     "cp2"
    ]
   },
   "outputs": [],
   "source": [
    "#Read first file content\n",
    "with open(f\"./{files[0]['filename']}\", \"r\") as file:\n",
    "  content = file.read()\n",
    "\n",
    "\n",
    "# Create a text splitter\n",
    "## YOUR SOLUTION HERE ##\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split the 'content' into chunks\n",
    "chunks = text_splitter.create_documents([content])\n",
    "\n",
    "# Print the first document\n",
    "chunks[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMicujI8Abvd"
   },
   "source": [
    "#### Checkpoint 3/4\n",
    "\n",
    "Now, you'll loop through each file, create chunks from the text splitter, and upload each to Chroma. Three empty lists are instantiated, which hold information about each document:\n",
    "- `documents` and `ids` are lists of strings holding the text and a unique identifier.  \n",
    "- `metadata` is a list of dictionaries which can hold metadata about each document, like the title, chunk index, and source URL.  \n",
    "\n",
    "We've already written the loop for this that appends the retrieved metadata and indices. Use the `page_content` attribute to append to `documents` within the loop.\n",
    "\n",
    " \n",
    "**Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oxQykt0YBGgk",
    "outputId": "4bdfc3e7-3761-4f3e-9372-a2ad8eb55c36",
    "tags": [
     "cp3"
    ]
   },
   "outputs": [],
   "source": [
    "#Create empty lists to store each document, metadata, and id\n",
    "documents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "\n",
    "#Loop through each file in files\n",
    "for file_info in files:\n",
    "    with open(f\"./{file_info['filename']}\", \"r\") as file:\n",
    "        content = file.read()\n",
    "        #Use text_splitter to create documents\n",
    "        chunks = text_splitter.create_documents([content])\n",
    "        #iterate over every chunk\n",
    "        for index, chunk in enumerate(chunks):\n",
    "            #Append to metadata list with \"title\", \"source_url\", and \"index\"\n",
    "            metadatas.append({\n",
    "                \"title\": file_info[\"title\"],\n",
    "                \"source_url\": file_info[\"source_url\"],\n",
    "                \"chunk_idx\": index\n",
    "            })\n",
    "            #Append to ids each index\n",
    "            ids.append(f\"{file_info['filename']}_{index}\")\n",
    "            \n",
    "            #Append to documents each chunk.page_content\n",
    "            ### YOUR SOLUTION HERE ###\n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoint 4/4\n",
    "\n",
    "Finally, pass to `.query()` a question about art or food in anthropology to verify that the database successfully accepted the chunks. \n",
    " \n",
    "**Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oxQykt0YBGgk",
    "outputId": "4bdfc3e7-3761-4f3e-9372-a2ad8eb55c36",
    "tags": [
     "cp4"
    ]
   },
   "outputs": [],
   "source": [
    "#Add all documents to the collection\n",
    "collection.add(documents=documents, metadatas=metadatas, ids=ids)\n",
    "\n",
    "#Verify documents were added to collection with a sample query\n",
    "### YOUR SOLUTION HERE ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary style=\"display:list-item; font-size:16px; color:blue;\"> Notice anything interesting? </summary>\n",
    "\n",
    "Notice that the results are returned as a dictionary and the cosine similarities for each result is also returned with the key `distances`.\n",
    "Consider experimenting with different queries and parameters and notice the change in your results. For example, how close (or far) are your results for unrelated queries?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuZUognqRpg8"
   },
   "source": [
    "Great job, we've solidified our knowledge by creating a persistent Chroma database, added documents, and queried the results -- we are ready to integrate these results within our Streamlit app!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
