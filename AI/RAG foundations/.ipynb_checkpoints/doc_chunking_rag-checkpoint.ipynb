{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8246b47c"
   },
   "source": [
    "<details><summary style=\"display:list-item; font-size:16px; color:blue;\">Jupyter Help</summary>\n",
    "    \n",
    "Having trouble testing your work? Double-check that you have followed the steps below to write, run, save, and test your code!\n",
    "    \n",
    "[Click here for a walkthrough GIF of the steps below](https://static-assets.codecademy.com/Courses/ds-python/jupyter-help.gif)\n",
    "\n",
    "Run all initial cells to import libraries and datasets. Then follow these steps for each question:\n",
    "    \n",
    "1. Add your solution to the cell with `## YOUR SOLUTION HERE ## `.\n",
    "2. Run the cell by selecting the `Run` button or the `Shift`+`Enter` keys.\n",
    "3. Save your work by selecting the `Save` button, the `command`+`s` keys (Mac), or `control`+`s` keys (Windows).\n",
    "4. Select the `Test Work` button at the bottom left to test your work.\n",
    "\n",
    "![Screenshot of the buttons at the top of a Jupyter Notebook. The Run and Save buttons are highlighted](https://static-assets.codecademy.com/Paths/ds-python/jupyter-buttons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae023367"
   },
   "source": [
    "**Setup**\n",
    "\n",
    "We'll start by loading in the text of the Wikipedia article on Llamas. Run the code cell below to import this file and assign it to the `content` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 825
    },
    "id": "a9378e93",
    "outputId": "31f833c3-aebc-4260-88eb-58b32018d0bc",
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# Scraped from https://en.wikipedia.org/wiki/Llama\n",
    "with open(\"Llama_Wikipedia_Cleaned.txt\", \"r\") as file:\n",
    "    content = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "079e0e0c"
   },
   "source": [
    "#### Checkpoint 1/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a8c74d0"
   },
   "source": [
    "Now we'll explore a few different strategies from chunking text, from the simplest to the most sophisticated.\n",
    "\n",
    "Let's first explore how to split text without the use of a high-level library. Below, we've written a simple function `split_content_into_equal_length` that chunks out our document in the most naive way possible. It takes two parameters: `content` and `char_length`. `char_length` is the length of the each chunk in characters. `content` is the text that we want to split.\n",
    "\n",
    "Using the function we defined, split the content into chunks of 300 characters each and return the first three chunks using the slice method.\n",
    "\n",
    "Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kzRiIVkxkA2z",
    "outputId": "2f076f49-00b6-4187-fcfb-0e7ca0a33419",
    "tags": [
     "cp1"
    ]
   },
   "outputs": [],
   "source": [
    "def split_content_into_equal_length(content, char_length):\n",
    "    return [content[i:i+char_length] for i in range(0, len(content), char_length)]\n",
    "\n",
    "split_content_into_equal_length(content, 300)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nz5AOQEO5Fbc"
   },
   "source": [
    "Read through the resulting chunks above. Can you identify the issues with this naive approach? How might these chunks be problematic for a language model trying to understand the content? Show the toggle below to see the answer.\n",
    "\n",
    "<details>By cutting off each chunk after a certain number of characters, we often cut words up before they're finished. And nearly all the time, the word the chunk cuts on comes before the end of a sentence. This abrupt start and finish to the chunks provides incomplete information to the language model when performing RAG. We need to find a better way! </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seBdXJdR5is7"
   },
   "source": [
    "While the naive approach is simple, it's clearly not ideal for natural language processing tasks. We could try splitting the text at sentence boundaries.\n",
    "\n",
    "In the cell below, we split the chunks every time we encounter a period and a space (`\". \"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9L4hxMl95qfr"
   },
   "outputs": [],
   "source": [
    "def simple_sentence_splitter(text, chunk_size=5):\n",
    "    # Split text based on a period followed by a space, which is a common end of sentence marker\n",
    "    sentences = text.split('. ')\n",
    "    chunks = []\n",
    "\n",
    "    # Group sentences into chunks of 'chunk_size'\n",
    "    for i in range(0, len(sentences), chunk_size):\n",
    "        chunk = '. '.join(sentences[i:i+chunk_size]) + '.'\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Example usage with the 'content' variable\n",
    "content_chunks = simple_sentence_splitter(content, 5)\n",
    "print(content_chunks[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qzXxIqs5zmV"
   },
   "source": [
    "This is an obvious improvement on the naive character splitting. However, it's got some obvious holes: what about question marks and other forms of sentence-ending punctuation, for instance?\n",
    "\n",
    "We also have yet to implement overlapping chunks, while the complexity of our code keeps increasing. Wouldn't it be nice if we could use a library that did this easily, so that we could prototype RAG applications faster? This is where higher-level libraries like LangChain come in, providing advanced functionality out of the box.\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. It provides a suite of tools for working with text, including advanced text splitters. You can find more information in the LangChain documentation: https://python.langchain.com/docs/modules/data_connection/document_transformers.\n",
    "\n",
    "We'll use Langchain's `RecursiveCharacterTextSplitter` to chunk our document.\n",
    "\n",
    "The `RecursiveCharacterTextSplitter` tries to split the text at the largest level first and progresses recursively through smaller and smaller splits. For instance, if it can break a document at the paragraph level, it will. If it doesn't find a break at its character limit at the paragraph level, it will resort to spllitting at the sentence level. It also supports creating overlapping chunks without any headaches.\n",
    "\n",
    "Let's start by importing the `RecursiveCharacterTextSplitter` class from the `langchain.text_splitter module` as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IL7YJmFI6ckV"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pM-WvVP6lDY"
   },
   "source": [
    "Next, we create an instance of the `RecursiveCharacterTextSplitter` class called `text_splitter`. We pass three arguments:\n",
    "\n",
    "- `separators`: A list of strings that the splitter will use to split the text. The splitter will try to split on these in order, moving to the next one if the current one is not found. Here, it's set to `[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \"]`\n",
    "- `chunk_size`: The maximum size of each chunk, in characters. Here, it's set to 600.\n",
    "- `chunk_overlap`: The number of characters of overlap between adjacent chunks. Here, it's set to 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lbzVI5J6rxj"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \"],  # List of characters to split on\n",
    "    chunk_size=600,  # The maximum size of your chunks\n",
    "    chunk_overlap=50,  # The maximum overlap between chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vn_Bk7Pc6wLH"
   },
   "source": [
    "We then use the `create_documents` method of our `text_splitter` instance to split the content string into chunks. This method takes a list of strings as input (in this case, a list with a single string, `content`), and returns a list of `Document` objects, each representing a chunk.\n",
    "\n",
    "Let's save this result in the variable `langchain_chunks` as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IQIZrpP62a3"
   },
   "outputs": [],
   "source": [
    "langchain_chunks = text_splitter.create_documents([content])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jz84K3rs667K"
   },
   "source": [
    "The result is a list of `document` objects each representing a chunk, we can look at each chunk using the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZhSXArk6-kT"
   },
   "outputs": [],
   "source": [
    "langchain_chunks[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a172e5e"
   },
   "source": [
    "#### Checkpoint 2/3\n",
    "\n",
    "Create an instance of the `RecursiveCharacterTextSplitter` class called `text_splitter` and use only two parameters: `chunk_size` and `chunk_overlap`. Set `chunk_size` to 1500 and set `chunk_overlap` to 300. Use the `create_documents` method to split `content` into chunks. Save this result in the variable `langchain_chunks` and print the first `document` object of the `langchain_chunks`.\n",
    "\n",
    "Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOfC9qjA8k84",
    "tags": [
     "cp2"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=300,\n",
    ")\n",
    "\n",
    "# Split the 'content' into chunks\n",
    "langchain_chunks = text_splitter.create_documents([content])\n",
    "\n",
    "# Print the first document\n",
    "langchain_chunks[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d481799"
   },
   "source": [
    "#### Checkpoint 3/3\n",
    "\n",
    "Next, you'll review what we covered last time and start up a new Chroma collection after instantiating the client. Name the collection `llama_chunks`.\n",
    "\n",
    "Then you'll loop through the `langchain_chunks` and upload each one to Chroma.\n",
    "\n",
    "You can ensure each id is unique by making its id some variation of the `index`. Access the text from the `Document` object and pass it to `documents`.\n",
    "\n",
    "You'll notice we've got a third argument we're passing this time, `metadatas`. Chroma allows you to attach metadata about each chunk when you add it. In this case, we'll add the chunk's `index` and a URL to the llama Wikipedia page it comes from.\n",
    "\n",
    "Finally, pass to `.query()` a question about llamas and see what you find out!\n",
    "\n",
    "Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qlQlKCM-vxa",
    "tags": [
     "cp3"
    ]
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Instantiate a Chroma client\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Create a new collection named \"llama_chunks\"\n",
    "collection = chroma_client.get_or_create_collection(name=\"llama_chunks\")\n",
    "\n",
    "\n",
    "# Iterate over the langchain_chunks using a for loop and enumerate\n",
    "for index, chunk in enumerate(langchain_chunks):\n",
    "    # Add a new document to the collection\n",
    "    collection.add(\n",
    "        ids=[f\"chunk_{index}\"],  # Specify the ids parameter as a list containing a single string in the format f\"chunk_{index}\"\n",
    "        documents=[chunk.page_content],  # Specify the documents parameter as a list containing a single string, which is the page_content of the current chunk\n",
    "        metadatas=[{\"source\": \"https://en.wikipedia.org/wiki/Llama\",\n",
    "                    \"chunk_index\": index}]  # Specify the metadatas parameter as a list containing a single dictionary with \"source\" and \"chunk_index\" keys\n",
    "    )\n",
    "\n",
    "# Query the collection\n",
    "results = collection.query(\n",
    "    query_texts=[\"What are llamas used for?\"],  # Specify the query_texts parameter as a list containing a single string, which is your query\n",
    "    n_results=1  # Specify the n_results parameter as an integer indicating the number of results you want\n",
    ")\n",
    "\n",
    "# Print the query results\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
